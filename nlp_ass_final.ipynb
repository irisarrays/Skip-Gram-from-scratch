{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_ass_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8GWn8im1k6dt"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "from sklearn.preprocessing import normalize\n",
        "import collections\n",
        "import pickle\n",
        "\n",
        "#__authors__ = ['author1','author2','author3']\n",
        "#__emails__  = ['fatherchristmoas@northpole.dk','toothfairy@blackforest.no','easterbunny@greenfield.de']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "xcZc5XS151le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text2sentences(path):\n",
        "    punctuations = {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
        "                    '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', \n",
        "                    ']', '^', '_', '`', '{', '|', '}', '~',' '}\n",
        "    \n",
        "    sentences = []\n",
        "    with open(path, encoding = 'utf-8') as f:\n",
        "        for l in f:\n",
        "            words = [''.join(ch for ch in word if ch not in punctuations) for word in l.lower().split()]\n",
        "            if len(words) > 1:  \n",
        "                sentences.append( words )\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "2SW0JtfUlEjA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadPairs(path):\n",
        "\tdata = pd.read_csv(path, delimiter='\\t')\n",
        "\tpairs = zip(data['word1'],data['word2'],data['similarity'])\n",
        "\treturn pairs"
      ],
      "metadata": {
        "id": "tfTCcDcalH-5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    if x > 5:\n",
        "        return 1\n",
        "    elif x < -5:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "jMPDjwabbBis"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Skip Gram"
      ],
      "metadata": {
        "id": "9G2gUZgg57AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "    def __init__(self, sentences=[], nEmbed=10, negativeRate=5, winSize=5, minCount=1):\n",
        "\n",
        "        # store parameters as class parameters\n",
        "        self.trainset = sentences\n",
        "        self.nEmbed = nEmbed\n",
        "        self.negativeRate = negativeRate\n",
        "        self.minCount = minCount\n",
        "        self.winSize = winSize\n",
        "\n",
        "        # set learning rate for back propogation process\n",
        "        self.lr = 0.002\n",
        "\n",
        "        # constrcut a dictionary where words are keys and corresponding frequencies are values\n",
        "        self.word_counts = collections.defaultdict(int)\n",
        "        for row in sentences:\n",
        "            for word in row:\n",
        "                self.word_counts[word] += 1\n",
        "\n",
        "        # filter words by mininal count\n",
        "        self.vocab = {k: v for k, v in self.word_counts.items() if v > self.minCount}\n",
        "\n",
        "        # reflect words as numbers\n",
        "        self.w2id = dict(zip(self.vocab.keys(), np.arange(0, len(self.vocab))))\n",
        "        id = self.w2id.values()\n",
        "\n",
        "        # store the number of words in corpus\n",
        "        self.count = len(self.vocab.keys())\n",
        "\n",
        "        # initialize the embedding matrix W and C\n",
        "        self.W = np.random.randn(self.count, nEmbed)\n",
        "        self.C = np.random.randn(self.count, nEmbed)\n",
        "\n",
        "        # initialize loss \n",
        "        self.loss = []\n",
        "        self.trainWords = 0\n",
        "        self.accLoss = 0.\n",
        "\n",
        "        # compute the probability of each word\n",
        "        self.prob = {}\n",
        "        total_count = 0\n",
        "        for w in self.w2id.keys():\n",
        "            # penalize the counts of all the words by power of 3/4\n",
        "            count = self.word_counts[w] ** (3 / 4)\n",
        "            total_count += count\n",
        "            self.prob[self.w2id[w]] = count\n",
        "        # dictionary with keys are ids and values are probability(frequency)\n",
        "        self.prob = {k: v / total_count for k, v in self.prob.items()}\n",
        "\n",
        "    def sample(self, omit, negativeRate = 5):\n",
        "        \n",
        "        \"\"\"\n",
        "        samples negative words, ommitting those in set omit.\n",
        "        Words that actually appear within the context window of the center word \n",
        "        and generate ids of words that are randomly drawn from a noise distribution\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        omit: tuple of {wIdx, ctxtId}\n",
        "        wIdx is the index of center word, ctxtId is the index of context word \n",
        "        \n",
        "        negetiveRate: a hyper-parameter that can be empircially tuned, the number of negative index\n",
        "        which will be sampled\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        negativeIds: a list of index of negative words\n",
        "        \"\"\"\n",
        "\n",
        "        # need to align with paper a bit\n",
        "        prob_list = list(self.prob.values())\n",
        "        id_list = list(self.prob.keys())\n",
        "\n",
        "        # randomly choose k(negativerate) ids of words based on their probabilites\n",
        "        negativeIds = np.random.choice(id_list, replace=False, size=self.negativeRate, p=prob_list)\n",
        "\n",
        "        # replace ones which are already in corpus\n",
        "        for i in range(negativeRate):\n",
        "            if negativeIds[i] in omit:\n",
        "                while negativeIds[i] in omit:\n",
        "                    negativeIds[i] = np.random.choice(id_list, p=prob_list)\n",
        "        return negativeIds\n",
        "\n",
        "    def train(self, nb_epochs=10):\n",
        "        eta = 0.25\n",
        "        for epoch in range(nb_epochs):\n",
        "            eta = 0.9 * eta\n",
        "            for counter, sentence in enumerate(self.trainset):\n",
        "                sentence = list(filter(lambda word: word in self.vocab, sentence))\n",
        "\n",
        "                for wpos, word in enumerate(sentence):\n",
        "                    wIdx = self.w2id[word]\n",
        "                    winsize = np.random.randint(self.winSize) + 1\n",
        "                    start = max(0, wpos - winsize)\n",
        "                    end = min(wpos + winsize + 1, len(sentence))\n",
        "                    for context_word in sentence[start:end]:\n",
        "                        ctxtId = self.w2id[context_word]\n",
        "                        if ctxtId == wIdx: continue\n",
        "                        negativeIds = self.sample({wIdx, ctxtId})\n",
        "                        self.trainWord(wIdx, ctxtId, negativeIds, eta)\n",
        "                        self.trainWords += 1\n",
        "                        self.accLoss += self.compute_loss(wIdx, ctxtId)\n",
        "                if counter % 100 == 0:\n",
        "                    # print(' > training %d of %d' % (counter, len(self.trainset)))\n",
        "                    self.loss.append(self.accLoss / self.trainWords)\n",
        "                    self.trainWords = 0\n",
        "                    self.accLoss = 0.\n",
        "\n",
        "    def trainWord(self, wordId, contextId, negativeIds, lr=0.002):\n",
        "\n",
        "        W_update = 0\n",
        "        W_update -= (sigmoid(np.dot(self.W[wordId,:], self.C[contextId, :])) - 1) * self.C[contextId, :]\n",
        "\n",
        "        for negativeId in negativeIds:\n",
        "            self.C[contextId, :] -= self.lr * sigmoid(np.dot(self.W[negativeId,:], self.C[contextId, :])) * self.W[wordId, :]\n",
        "            W_update -= sigmoid(np.dot(self.W[negativeId,:], self.C[contextId, :])) * self.C[contextId, :]\n",
        "\n",
        "        self.W[wordId, :] += self.lr * W_update\n",
        "\n",
        "    def similarity(self, word1, word2, nEmbed=10):\n",
        "        \n",
        "        common_vect = +np.ones(self.nEmbed) * 10000\n",
        "        if word1 not in self.vocab and word2 in self.vocab:\n",
        "            id_word_2 = self.w2id[word2]\n",
        "            w1 = common_vect\n",
        "            w2 = self.W[id_word_2]\n",
        "        elif word1 in self.vocab and word2 not in self.vocab:\n",
        "            id_word_1 = self.w2id[word1]\n",
        "            w1 = self.W[id_word_1]\n",
        "            w2 = common_vect\n",
        "        elif word1 not in self.vocab and word2 not in self.vocab:\n",
        "            w1 = common_vect\n",
        "            w2 = common_vect\n",
        "        else:\n",
        "            id_word_1 = self.w2id[word1]\n",
        "            id_word_2 = self.w2id[word2]\n",
        "            w1 = self.W[id_word_1]\n",
        "            w2 = self.W[id_word_2]\n",
        "        similarity = w1.dot(w2) / (np.linalg.norm(w1) * np.linalg.norm(w2))\n",
        "        return similarity\n",
        "\n",
        "    def compute_loss(self, wordId_1, wordId_2):\n",
        "\n",
        "        # get the vectors of both of the words\n",
        "        w1 = self.W[wordId_1]\n",
        "        w2 = self.W[wordId_2]\n",
        "\n",
        "        scalar = w1.dot(w2)\n",
        "        similarity = 1 / (1 + np.exp(-scalar))\n",
        "        return similarity\n",
        "    \n",
        "    def save(self, path):\n",
        "      pickle.dump(self, open(path, 'wb'))\n",
        "    \n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "      return pickle.load(open(path, 'rb'))\n"
      ],
      "metadata": {
        "id": "Nbl0kxemlIrG"
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIG DATASET"
      ],
      "metadata": {
        "id": "6yaHHcHRYi2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for a complete training set\n",
        "sentences = text2sentences('train.txt')\n",
        "sg = SkipGram(sentences, nEmbed=10, negativeRate=5, winSize=5, minCount=1)\n",
        "sg.train()\n",
        "sg.save(\"/content/sample_data/mymodel.model\")"
      ],
      "metadata": {
        "id": "i62VxjIYal0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = loadPairs('simlex.csv')\n",
        "sg = SkipGram.load('/content/sample_data/mymodel.model')\n",
        "\n",
        "predict=[]\n",
        "true = []\n",
        "for a, b, true_score in pairs:\n",
        "  pred = sg.similarity(a,b)\n",
        "  predict.append(pred)\n",
        "  true.append(true_score)\n",
        "  \n",
        "print('correlation result is', pd.Series(predict).corr(pd.Series(true)))"
      ],
      "metadata": {
        "id": "hmm-wGJuYh6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMALL DATASET"
      ],
      "metadata": {
        "id": "Mv2HFbwpYl61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for small trainning set\n",
        "sentences = text2sentences('train_1.txt')\n",
        "sg = SkipGram(sentences, nEmbed=10, negativeRate=5, winSize=5, minCount=1)\n",
        "sg.train()\n",
        "sg.save(\"/content/sample_data/mymodel.model\")"
      ],
      "metadata": {
        "id": "Bo8BLeOeHJGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = loadPairs('simlex.csv')\n",
        "sg = SkipGram.load('/content/sample_data/mymodel.model')\n",
        "predict=[]\n",
        "true = []\n",
        "for a, b, true_score in pairs:\n",
        "  pred = sg.similarity(a,b)\n",
        "  predict.append(pred)\n",
        "  true.append(true_score)\n",
        "print(len(true))\n",
        "print(len(predict))\n",
        "print('correlation result is(TESTING)', pd.Series(predict).corr(pd.Series(true)))"
      ],
      "metadata": {
        "id": "UMcAH1jdcZUC"
      },
      "execution_count": 464,
      "outputs": []
    }
  ]
}